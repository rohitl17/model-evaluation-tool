{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Imports\n",
    "2. Model initialization and loading\n",
    "\n",
    "In-sample set: \n",
    "    1. Model predictions and storing Image_top10%prediction dictionary/csv ✔\n",
    "    2. Use the actual threshold given by the data scientist to categorize or classify images (TP, FP, FN, TN) ✔\n",
    "    3. Find out best threshold and fill up respective columns \n",
    "    4. Segregate images into positive and negative on basis of GT and arrange images in descending order of probabilities, fill up the columns\n",
    "    5. Tag the images with the buckets and Use 50 top, 50 bottom and 50 randomly sampled images from the middle bucket and create a separate CSV\n",
    "    6. Plot images GT, Radiologist Annotation and AI prediction and use interactive fields to update  the CSV with Radiologist comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and DS_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/bucketdata/Users/Rohit/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import cv2\n",
    "import pydicom          \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout,\\\n",
    "    UpSampling2D, LeakyReLU, Add, ZeroPadding2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "import ds_utils\n",
    "from ds_utils import utils\n",
    "\n",
    "ds_utils.set_framework(\"keras\")\n",
    "print(ds_utils.framework())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_Audits.normal_abnormal_2.scripts import pathology_pipeline_2 as model_pipeline\n",
    "# from Model_Audits.normal_abnormal_1.scripts import abnormality_pipeline as abnormality_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_groundtruth(region_attribute, mask_temp, color=(255,255,255)):\n",
    "    try:\n",
    "        x = region_attribute['x']\n",
    "        y = region_attribute['y']\n",
    "        w = region_attribute['width']\n",
    "        h = region_attribute['height']\n",
    "        ground_truth_mask=cv2.rectangle(mask_temp,(int(x),int(y)),(int(x)+int(w),int(y)+int(h)),color,5)\n",
    "    except:\n",
    "        x = list(region_attribute['x'])\n",
    "        y = list(region_attribute['y'])\n",
    "        pts = [list(k) for k in zip(x, y)]\n",
    "        pts = np.array(pts,np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "#         ground_truth_mask=cv2.fillPoly(mask_temp, [pts], pathology_color_dict[patho]) #  To get filled masks\n",
    "        ground_truth_mask = cv2.polylines(mask_temp, [pts], True, color, 16) # To get boundaries of mask\n",
    " \n",
    "    return (ground_truth_mask)\n",
    "\n",
    "\n",
    "def preprocess_wrapper(region_attributes, mask, color=(255,255,255)):\n",
    "    for i in region_attributes:\n",
    "        try:\n",
    "            mask=preprocess_groundtruth(i, mask, color)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "#     mask=cv2.resize(mask, (model_pipeline.img_size, model_pipeline.img_size))\n",
    "#     mask=np.where(mask>0,255,0)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_table(threshold, ground_truth, ai_probability):\n",
    "    metrics = []\n",
    "    metrics.append(utils.compute_metrics(ground_truth, np.where(ai_probability > threshold,1,0), print_metrics=False))\n",
    "    metrics[-1][\"roc_auc\"] = roc_auc_score(ground_truth, ai_probability)\n",
    "    df_2 = pd.DataFrame(metrics)\n",
    "    df_2 = df_2.applymap(\"{0:.3f}\".format)\n",
    "    \n",
    "    matrix=confusion_matrix(ground_truth, np.where(ai_probability > threshold,1,0))\n",
    "    df_2['true_positives']=matrix[1][1]\n",
    "    df_2['true_negatives']=matrix[0][0]\n",
    "    df_2['false_positives']=matrix[0][1]\n",
    "    df_2['false_negatives']=matrix[1][0]\n",
    "    return (df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_required_directories(output_path):\n",
    "    os.makedirs(output_path+'heatmaps/')\n",
    "    os.makedirs(output_path+'predicted_masks/')\n",
    "    os.makedirs(output_path+'ground_truth_pathology_nopathology/')\n",
    "    os.makedirs(output_path+'original_images/')\n",
    "    os.makedirs(output_path+'ground_truth_all_pathologies/')\n",
    "    os.makedirs(output_path+'abnormality_masks/')\n",
    "    os.makedirs(output_path+'legends/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_legends(groundtruth_csv, image_name):\n",
    "    \n",
    "    specific_row=groundtruth_csv.loc[groundtruth_csv['image_path']==i]\n",
    "    pathologies=[]\n",
    "    \n",
    "    for j in specific_row.columns[9:]:\n",
    "        if len(specific_row[j].values[0])>6:\n",
    "            pathologies.append(j)\n",
    "            \n",
    "    count=0\n",
    "    image=np.ones((768,1536,3),dtype='uint8')\n",
    "    image=image*100\n",
    "    for ldx, l in enumerate(list(sushrut_pathology_color_mapping.keys())):\n",
    "        if l in pathologies:\n",
    "            count+=1\n",
    "            if count<7:\n",
    "                image=cv2.putText(image,\"---\"+l, (10,100*count), cv2.FONT_HERSHEY_SIMPLEX, 3, sushrut_pathology_color_mapping[l], 2)\n",
    "            else:\n",
    "                image=cv2.putText(image,\"---\"+l, (700,100*((count+1)%7)), cv2.FONT_HERSHEY_SIMPLEX, 3, sushrut_pathology_color_mapping[l], 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_mask(name, image, groundtruth_csv): #All the pathologies\n",
    "    specific_row=groundtruth_csv.loc[groundtruth_csv['image_path']==name]\n",
    "    \n",
    "    semi_gt_mask=np.zeros((image.shape[0], image.shape[1], 3), dtype='uint8')\n",
    "    \n",
    "    for j in specific_row.columns[9:]:\n",
    "        if len(specific_row[j].values[0])>6:\n",
    "            if j in sushrut_pathology_color_mapping:\n",
    "#             try:\n",
    "                print (j, specific_row[j].values[0])\n",
    "                semi_gt_mask=preprocess_wrapper(eval(specific_row[j].values[0]), semi_gt_mask, sushrut_pathology_color_mapping[j])\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "    semi_gt_mask=semi_gt_mask.astype('uint8')\n",
    "\n",
    "    gt_mask=semi_gt_mask\n",
    "#     print (\"GT Mask\", gt_mask.dtype, np.max(gt_mask), gt_mask.shape)\n",
    "    \n",
    "    gt_mask_final=cv2.addWeighted(image, 1, gt_mask, 1, 0, dtype=0)\n",
    "    \n",
    "    cv2.imwrite(output_path+'ground_truth_all_pathologies/'+name.split('/')[-1], cv2.resize(gt_mask_final, (768,768)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for idx, i in enumerate(filenames):\n",
    "    print (idx, i)\n",
    "    image=cv2.imread(base_path+i)\n",
    "    print (\"Image:\", image.shape)\n",
    "    ground_truth_mask(i, image, ground_truth_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='/opt/bucketdata/ORIGINAL_DATA/MASTER_DATA/'\n",
    "pathology='normal'                                                         # Name of the column with Ground truth Annotations\n",
    "output_path='../saved_images/'\n",
    "parent_directory='../'\n",
    "\n",
    "IMG_SIZE=768\n",
    "\n",
    "sushrut_pathology_color_mapping={\n",
    "'pleural effusion':(255, 255,255),\n",
    "'pneumonia':(0,255,0),\n",
    "'covid':(0,255,0),\n",
    "'edema':(0,0,255),\n",
    "'atelectasis':(0,255,255),\n",
    "'tuberculosis': (255, 0, 255),\n",
    "'fibrosis': (236, 252, 0),\n",
    "'cardiomegaly': (25, 110, 230),\n",
    "'lung mass': (100, 0, 0),\n",
    "'surgical emphysema': (0, 100, 0),\n",
    "'nodule': (0, 0, 100),\n",
    "'pneumothorax': (0, 57, 122),\n",
    "'scoliosis': (120,0,120),\n",
    "'opaque hemithorax': (146, 156, 0),\n",
    "'hernia': (255,255,255),\n",
    "'pleural thickening': (125,125,125)\n",
    "}\n",
    "\n",
    "pathology=list(sushrut_pathology_color_mapping.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV from Sushrut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth_csv=pd.read_csv(parent_directory+'/miscellaneous/output.csv')\n",
    "df_test=pd.read_csv(parent_directory+'/miscellaneous/output.csv')\n",
    "filenames=list(df_test['image_path'])\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.sample(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=list(df_test['image_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and weights from Developer's script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_pipeline.abnormality_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction probability and other details for each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filenames=filenames[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AI_prediction=[]\n",
    "AI_probability=[]\n",
    "Ground_truth=[]\n",
    "Prediction_class=[]\n",
    "\n",
    "# make_required_directories(output_path)\n",
    "print (\"Directories created\")\n",
    "\n",
    "for idx, i in enumerate(filenames):\n",
    "    flag=0\n",
    "\n",
    "    try:\n",
    "        image=cv2.imread(base_path+i)\n",
    "        print (image.shape)\n",
    "    except:\n",
    "        image=cv2.imread(base_path+'BIMCV_disk/'+i)\n",
    "        flag=1\n",
    "        print (image.shape)\n",
    "    \n",
    "    semi_gt_mask=np.zeros((image.shape[0], image.shape[1]), dtype='uint8')\n",
    "#     ###if len(ast.literal_eval(df_test.loc[df_test['image_path']==i][pathology].values[0]))>0:\n",
    "#     semi_gt_mask=preprocess_wrapper(ast.literal_eval(df_test.loc[df_test['image_path']==i][pathology].values[0]), semi_gt_mask)\n",
    "#     semi_gt_mask=semi_gt_mask.astype('uint8')\n",
    "        \n",
    "#     if 1 in ast.literal_eval(df_test.loc[df_test['image_path']==i]['normal'].values[0]):\n",
    "#         Ground_truth.append(0)\n",
    "#     else:\n",
    "#         Ground_truth.append(1) \n",
    "\n",
    "    for j in pathology:\n",
    "        try:\n",
    "            if len(ast.literal_eval(df_test.loc[df_test['image_path']==i][j].values[0]))>0:\n",
    "                flag=1\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    if flag==0:\n",
    "        Ground_truth.append(0)\n",
    "    else:\n",
    "        Ground_truth.append(1)\n",
    "    \n",
    "    gt_mask=np.zeros((semi_gt_mask.shape[0],semi_gt_mask.shape[1],3), dtype='uint8')\n",
    "    gt_mask[:,:,0]=semi_gt_mask\n",
    "    gt_mask[:,:,1]=semi_gt_mask\n",
    "    gt_mask[:,:,2]=semi_gt_mask\n",
    "    \n",
    "    gt_mask=cv2.addWeighted(image, 0.7, gt_mask, 0.5, 0)\n",
    "    gt_mask=cv2.resize(gt_mask, (768,768))\n",
    "#  ##   print (\"GT Mask: \", gt_mask.dtype, np.max(gt_mask), gt_mask.shape)\n",
    "\n",
    "    ground_truth_mask(i, image, ground_truth_csv)\n",
    "    \n",
    "    image=cv2.resize(image, (768,768))\n",
    "    \n",
    "    print (\"Predicting on Image No: {0}, Filepath: {1}\".format(idx,(base_path+i)) )\n",
    "    prediction=model.predict(model_pipeline.preprocessing(image), verbose=1)\n",
    "    print (\"Model Inference done. Adding Entry to the Excel sheet\")\n",
    "    \n",
    "    original_threshold, predicted_probability, predicted_class, predicted_mask, heatmap = model_pipeline.postprocessing(prediction) \n",
    "    \n",
    "   ### predicted_mask=np.reshape(predicted_mask, (768,768,1))\n",
    "    print (\"Predicted Mask\", predicted_mask.dtype, np.max(predicted_mask), predicted_mask.shape)\n",
    "    \n",
    "    \n",
    "    predicted_mask=utils.overlay_mask_on_image(image, predicted_mask)\n",
    "\n",
    "    \n",
    "    AI_probability.append(predicted_probability)    \n",
    "    AI_prediction.append(predicted_class)\n",
    "\n",
    "    print (\"Heatmap Mask\", heatmap.dtype, np.max(heatmap), heatmap.shape)\n",
    "\n",
    "    heatmap = cv2.addWeighted(image,1.0,heatmap,0.2,0,dtype=0)\n",
    "    \n",
    "    if AI_prediction[-1]==1 and Ground_truth[-1]==1:\n",
    "        Prediction_class.append('True Positive')\n",
    "    elif AI_prediction[-1]==0 and Ground_truth[-1]==1:\n",
    "        Prediction_class.append('False Negative')\n",
    "    elif AI_prediction[-1]==1 and Ground_truth[-1]==0:\n",
    "        Prediction_class.append('False Positive')   \n",
    "    else:\n",
    "        Prediction_class.append('True Negative') \n",
    "        \n",
    "    print (\"Image\", image.dtype, np.max(image), image.shape)\n",
    "    \n",
    "    cv2.imwrite(output_path+'heatmaps/'+i.split('/')[-1], heatmap)\n",
    "    cv2.imwrite(output_path+'predicted_masks/'+i.split('/')[-1], predicted_mask)\n",
    "    cv2.imwrite(output_path+'ground_truth_pathology_nopathology/'+i.split('/')[-1], gt_mask)\n",
    "    cv2.imwrite(output_path+'original_images/'+i.split('/')[-1], image)\n",
    "            \n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['image_name']=filenames\n",
    "df['AI_probability']=AI_probability\n",
    "df['AI_prediction_original_threshold']=AI_prediction\n",
    "df['Ground_truth']=Ground_truth\n",
    "df['Original_threshold']=[original_threshold]*len(Ground_truth)\n",
    "df['AI_Prediction_Class']=Prediction_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=pd.DataFrame()\n",
    "df['image_name']=filenames[0:2500]\n",
    "df['AI_probability']=AI_probability[0:2500]\n",
    "df['AI_prediction_original_threshold']=AI_prediction[0:2500]\n",
    "df['Ground_truth']=Ground_truth[0:2500]\n",
    "df['Original_threshold']=[original_threshold]*2500\n",
    "df['AI_Prediction_Class']=Prediction_class[0:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! rm -rf pleural_effusion/saved_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(parent_directory+'/predicted_csvs/')\n",
    "df.to_csv(parent_directory+'/predicted_csvs/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, i in enumerate(filenames[0:2500]):\n",
    "    print(idx, i)\n",
    "#     image=cv2.imread(base_path+i)\n",
    "    \n",
    "#     image=cv2.resize(image, (512,512))\n",
    "    \n",
    "#     _,_,_,abnormality_mask=get_abnormality_prediction(image, model)\n",
    "    \n",
    "#     print (\"Predicted Masks\", abnormality_mask.dtype, np.max(abnormality_mask), abnormality_mask.shape)\n",
    "#     abnormality_mask=utils.overlay_mask_on_image(image, abnormality_mask)\n",
    "    legend=get_legends(ground_truth_csv, i)    \n",
    "    \n",
    "    cv2.imwrite(output_path+'legends/'+i.split('/')[-1], legend)\n",
    "#     cv2.imwrite(output_path+'abnormality_masks/'+i.split('/')[-1], cv2.resize(abnormality_mask, (768,768)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(parent_directory+'/predicted_csvs/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "a=widgets.interact(lambda x: get_results_table(x, list(df['Ground_truth']), (df['AI_probability'])),\n",
    "                         x=widgets.FloatSlider(min=0, max=1, step=0.01, value=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold=widgets.BoundedFloatText(\n",
    "    value=7.5,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.001,\n",
    "    description='Best Threshold:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The best threshold is: \",best_threshold.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Best_Threshold']=[best_threshold.value]*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(parent_directory+'/predicted_csvs/prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segregate Dataframes into positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns present: Image_name, AI_probability, AI_prediction_original_threshold, Ground_truth, Original_threshold\n",
    "# Columns to be added: Percentile as per the class\n",
    "\n",
    "df_positive=df.loc[df['Ground_truth']==1]\n",
    "df_negative=df.loc[df['Ground_truth']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive=df_positive.sort_values(by=['AI_probability'], ascending='True')\n",
    "df_negative=df_negative.sort_values(by=['AI_probability'], ascending='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_positive=pd.concat([df_positive, df_negative])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Percentile and AI confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_percentile=[]\n",
    "ai_confidence=[]\n",
    "i=0\n",
    "while i<len(df_positive):\n",
    "#     print (i)\n",
    "    percentile=(i/len(df_positive))*100\n",
    "    positive_percentile.append(percentile)\n",
    "    \n",
    "    if percentile>70:\n",
    "        ai_confidence.append(\"High\")\n",
    "    elif percentile>30:\n",
    "        ai_confidence.append(\"Medium\")\n",
    "    else:\n",
    "        ai_confidence.append(\"Low\")\n",
    "        \n",
    "    i=i+1\n",
    "    \n",
    "df_positive['Percentile']=positive_percentile\n",
    "df_positive['AI_Confidence']=ai_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_percentile=[]\n",
    "ai_confidence=[]\n",
    "i=0\n",
    "while i<len(df_negative):\n",
    "    negative_percentile.append((1-i/len(df_negative))*100)\n",
    "    \n",
    "    if ((1-i/len(df_negative))*100)>70:\n",
    "        ai_confidence.append(\"High\")\n",
    "    elif ((1-i/len(df_negative))*100)>30:\n",
    "        ai_confidence.append(\"Medium\")\n",
    "    else:\n",
    "        ai_confidence.append(\"Low\")\n",
    "        \n",
    "    i=i+1\n",
    "    \n",
    "df_negative['Percentile']=negative_percentile\n",
    "df_negative['AI_Confidence']=ai_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images According to the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive images\n",
    "top_positive_bin=df_positive.tail(num_samples)\n",
    "mid_positive_bin=df_positive[int(len(df_positive)*0.4):int(len(df_positive)*0.7)].sample(num_samples)\n",
    "bottom_positive_bin=df_positive.head(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Images\n",
    "\n",
    "top_negative_bin=df_negative.head(num_samples)\n",
    "mid_negative_bin=df_negative[int(len(df_negative)*0.4):int(len(df_negative)*0.7)].sample(num_samples)\n",
    "bottom_negative_bin=df_negative.tail(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_negative_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save the Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grids_and_save_images(bin_name, image_names, save_path):\n",
    "    os.makedirs(save_path)\n",
    "    for idx, i in enumerate(image_names):\n",
    "        print(bin_name, str(idx*100/len(image_names))+\"% Done\")\n",
    "        original_image=cv2.imread(output_path+'/original_images/'+i.split('/')[-1])\n",
    "        ground_truth=cv2.imread(output_path+'/ground_truth_all_pathologies/'+i.split('/')[-1])\n",
    "        abnormality_mask=cv2.imread(output_path+'/predicted_masks/'+i.split('/')[-1])\n",
    "        heatmap=cv2.imread(output_path+'/heatmaps/'+i.split('/')[-1])\n",
    "        legend=cv2.imread(output_path+'/legends/'+i.split('/')[-1])\n",
    "\n",
    "        upper = np.concatenate((original_image, ground_truth), axis=1)\n",
    "        lower = np.concatenate((abnormality_mask, heatmap), axis=1)\n",
    "        final=np.concatenate((upper,lower), axis=0)\n",
    "        \n",
    "        final_grid=final\n",
    "#         final_grid=np.concatenate((final,legend), axis=0)    \n",
    "        cv2.imwrite(save_path+i.split('/')[-1], final_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grids_and_save_images(\"Top Positive Bin\", list(top_positive_bin['image_name']), parent_directory+'/radiologist_audit/highly_confident_positive_images/' )\n",
    "plot_grids_and_save_images(\"Medium Positive Bin\", list(mid_positive_bin['image_name']), parent_directory+'/radiologist_audit/medium_confident_positive_images/' )\n",
    "plot_grids_and_save_images(\"Least Positive Bin\", list(bottom_positive_bin['image_name']), parent_directory+'/radiologist_audit/low_confident_positive_images/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grids_and_save_images(\"Top Negative Bin\", list(top_negative_bin['image_name']), parent_directory+'/radiologist_audit/highly_confident_negative_images/' )\n",
    "plot_grids_and_save_images(\"Medium Negative Bin\", list(mid_negative_bin['image_name']), parent_directory+'/radiologist_audit/medium_confident_negative_images/' )\n",
    "plot_grids_and_save_images(\"Least Negative Bin\", list(bottom_negative_bin['image_name']), parent_directory+'/radiologist_audit/low_confident_negative_images/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
